{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'filename': r'data\\ubuntu dialogue\\Ubuntu-dialogue-corpus\\dialogueText.csv',\n",
    "        'question_length_threshold': 20,\n",
    "        'answer_length_threshold': 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing utterances...\n",
      "Done. 1038324 utterances parsed.\n",
      "Parsing into dialogues\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1038324/1038324 [00:01<00:00, 713331.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed utterances into 346108 distinct dialogues\n",
      "There are 0 non-three-turn dialogues\n",
      "There are 346108 three-turn dialogues\n",
      "Among the three-turn dialogues, there are 18881 with <=20 question words (in threshold), and 327227 over threshold\n",
      "Among the three-turn dialogues, there are 60173 with <=20 answer words (in threshold), and 285935 over threshold\n",
      "Total Number of Dialogues Falling Within the specified Thresholds: 3821\n",
      "Getting Word Counts\n",
      "Analyse.py ceased executing at 2023-07-27 01:16:22\n",
      "Shell output logged to ./output/20230727_011535.txt\n"
     ]
    }
   ],
   "source": [
    "class Dialogue:\n",
    "    id = 0,\n",
    "    utterances = [],\n",
    "    questionCount = 0,\n",
    "    combinedQuestionWordLength = 0,\n",
    "    combinedAnswerWordLength = 0,\n",
    "    answerCount = 0\n",
    "\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "        self.utterances = []\n",
    "        self.questionCount = 0\n",
    "        self.combinedQuestionWordLength = 0\n",
    "        self.combinedAnswerWordLength = 0\n",
    "        self.answerCount = 0\n",
    "\n",
    "class Utterance:\n",
    "    text = \"\",\n",
    "    dialogueId = -1,\n",
    "    fromUser = \"\",\n",
    "    isQuestion = False,\n",
    "    isAnswer = False\n",
    "\n",
    "    def __init__(self, text, dialogueId, fromUser):\n",
    "        self.text = text\n",
    "        self.dialogueId = dialogueId\n",
    "        self.fromUser = fromUser\n",
    "        self.isQuestion = False\n",
    "        self.isAnswer = False\n",
    "\n",
    "#region Logger Setup\n",
    "\n",
    "# Log to console, and to a timestamped log file\n",
    "def Log(text):    \n",
    "    print(text)\n",
    "    with open(logFilePath, 'a') as f:\n",
    "        f.write(datetime.now().strftime(\"%H:%M:%S\") + \" \" + text + \"\\n\")\n",
    "\n",
    "#endregion\n",
    "\n",
    "#region Setup\n",
    "\n",
    "fileName = args['filename']\n",
    "questionWordThreshold = args['question_length_threshold']\n",
    "answerWordThreshold = args['answer_length_threshold']\n",
    "\n",
    "if not os.path.exists(\"./output\"):\n",
    "    os.makedirs(\"./output\")\n",
    "\n",
    "startTime = datetime.now()\n",
    "logFileName = startTime.strftime(\"%Y%m%d_%H%M%S.txt\")\n",
    "logFilePath = \"./output/{logFileName}\".format(logFileName = logFileName)\n",
    "\n",
    "totalQuestionCount = 0\n",
    "totalAnswerCount = 0\n",
    "total3TurnDialogueCount = 0\n",
    "totalNon3TurnDialogueCount = 0\n",
    "totalLTEQuestionThresholdWord3TurnDialogueCount = 0\n",
    "totalOverQuestionThresholdWord3TurnDialogueCount = 0\n",
    "totalLTEAnswerThresholdWord3TurnDialogueCount = 0\n",
    "totalOverAnswerThresholdWord3TurnDialogueCount = 0\n",
    "totalDialoguesWithinThreshold = 0\n",
    "\n",
    "df = (pd.read_csv(fileName))\n",
    "\n",
    "#endregion\n",
    "\n",
    "#region Data Handling\n",
    "\n",
    "def GetUtterances():\n",
    "    parsedUtterances = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        dialogueId = row[\"dialogueID\"]\n",
    "        # Strip .tsv from end of dialogue Id\n",
    "        dialogueId = dialogueId[0:len(dialogueId) - 4]\n",
    "\n",
    "        # Make unique id from folder and dialogue id\n",
    "        strUniqueId = \"{folderId}{dialogueId}\".format(folderId = row[\"folder\"], dialogueId = dialogueId)\n",
    "        parsedUtterances.append(Utterance(row['text'], int(strUniqueId), row['from']))\n",
    "\n",
    "    return parsedUtterances\n",
    "\n",
    "#endregion\n",
    "\n",
    "#region Data Analysis\n",
    "\n",
    "# def PerformAnalysis():\n",
    "#     Log(\"Proceeding with analysis tasks...\")\n",
    "\n",
    "#     startTime = datetime.now()\n",
    "\n",
    "#     Log(\"Commencing analysis of {commentClassDescription} class comments\".format(commentClassDescription = commentClass.description))\n",
    "\n",
    "#     for comment in tqdm(commentClass.comments):\n",
    "#         TokeniseForAnalysis(comment, commentClass)\n",
    "\n",
    "#     Log(\"Spelling corrections required for {count} words\".format(count = commentClass.correctedSpellingsCount))\n",
    "#     Log(\"Sentence count: {count}\".format(count = commentClass.sentenceCount))\n",
    "#     Log(\"Token counts - before processing: {preTokensCount}, after processing: {postTokensCount} \".format(\n",
    "#         preTokensCount = commentClass.preProcessedTokenCount, postTokensCount = commentClass.postProcessedTokenCount))\n",
    "#     Log(\"Most commonly-appearing words: {top10}\".format(top10 = Counter(commentClass.tokens).most_common(10)))\n",
    "\n",
    "#     endTime = datetime.now()\n",
    "#     secondsElapsed = str(endTime - startTime)\n",
    "#     Log(\"Finished analysing '{commentClassDescription}' class in {elapsed}\".format(\n",
    "#         commentClassDescription = commentClass.description, elapsed = secondsElapsed))\n",
    "    \n",
    "#     Log(\"Analysis complete.\")\n",
    "\n",
    "# endregion\n",
    "\n",
    "# region Program Flow\n",
    "\n",
    "Log(\"Parsing utterances...\")\n",
    "utterances:List[Utterance] = GetUtterances()\n",
    "Log(\"Done. {count} utterances parsed.\".format(count = len(utterances)))\n",
    "\n",
    "Log(\"Parsing into dialogues\")\n",
    "dialogues:List[Dialogue] = []\n",
    "dialogue = Dialogue(utterances[0].dialogueId)\n",
    "lastDialogueId = utterances[0].dialogueId\n",
    "\n",
    "for u in tqdm(utterances):\n",
    "    if u.dialogueId != lastDialogueId:\n",
    "        # Stash the current dialogue and create a new one to work with\n",
    "        dialogues.append(dialogue)\n",
    "        dialogue = Dialogue(u.dialogueId)\n",
    "\n",
    "    # NOTE: Following our meeting 26/7/23, this logic is questionable\n",
    "    if len(dialogue.utterances) == 0:\n",
    "        # must the the question, first message\n",
    "        u.isQuestion = True\n",
    "    elif len(dialogue.utterances) == 1:\n",
    "        u.isQuestion = (dialogue.utterances[0].fromUser == u.fromUser)\n",
    "        u.isAnswer = (dialogue.utterances[0].fromUser != u.fromUser)\n",
    "    else:\n",
    "        # third turn cannot be the question\n",
    "        u.isQuestion = False\n",
    "        u.isAnswer = True\n",
    "\n",
    "    dialogue.utterances.append(u)\n",
    "\n",
    "    if u.isQuestion:\n",
    "        dialogue.questionCount += 1\n",
    "        totalQuestionCount += 1\n",
    "        dialogue.combinedQuestionWordLength += len(str(u.text))\n",
    "\n",
    "    if u.isAnswer:\n",
    "        dialogue.answerCount += 1\n",
    "        totalAnswerCount += 1\n",
    "        dialogue.combinedAnswerWordLength += len(str(u.text))\n",
    "\n",
    "    lastDialogueId = u.dialogueId\n",
    "\n",
    "# now push the final dialogue we were working on\n",
    "dialogues.append(dialogue)\n",
    "\n",
    "Log(\"Parsed utterances into {count} distinct dialogues\".format(count = len(dialogues)))\n",
    "\n",
    "for dialogue in dialogues:\n",
    "    if len(dialogue.utterances) == 3:\n",
    "        total3TurnDialogueCount += 1\n",
    "        if dialogue.combinedQuestionWordLength <= questionWordThreshold:\n",
    "            totalLTEQuestionThresholdWord3TurnDialogueCount += 1\n",
    "        else:\n",
    "            totalOverQuestionThresholdWord3TurnDialogueCount += 1\n",
    "\n",
    "        if dialogue.combinedAnswerWordLength <= answerWordThreshold:\n",
    "            totalLTEAnswerThresholdWord3TurnDialogueCount += 1\n",
    "        else:\n",
    "            totalOverAnswerThresholdWord3TurnDialogueCount += 1\n",
    "\n",
    "        if dialogue.combinedQuestionWordLength <= questionWordThreshold and dialogue.combinedAnswerWordLength <= answerWordThreshold:\n",
    "            totalDialoguesWithinThreshold += 1\n",
    "    else:\n",
    "        totalNon3TurnDialogueCount += 1\n",
    "        Log(\"Non 3 Turn Dialogue found, Our Unique ID: {id}\".format(id=dialogue.id))\n",
    "\n",
    "Log(\"There are {count} non-three-turn dialogues\".format(count = totalNon3TurnDialogueCount))\n",
    "Log(\"There are {count} three-turn dialogues\".format(count = total3TurnDialogueCount))\n",
    "Log(\"Among the three-turn dialogues, there are {count} with <={threshold} question words (in threshold), and {count2} over threshold\"\n",
    "    .format(count = totalLTEQuestionThresholdWord3TurnDialogueCount, count2 = totalOverQuestionThresholdWord3TurnDialogueCount, threshold=questionWordThreshold))\n",
    "Log(\"Among the three-turn dialogues, there are {count} with <={threshold} answer words (in threshold), and {count2} over threshold\"\n",
    "    .format(count = totalLTEAnswerThresholdWord3TurnDialogueCount, count2 = totalOverAnswerThresholdWord3TurnDialogueCount, threshold=answerWordThreshold))\n",
    "Log(\"Total Number of Dialogues Falling Within the specified Thresholds: {count}\".format(count=totalDialoguesWithinThreshold))\n",
    "\n",
    "Log(\"Getting Word Counts\")\n",
    "# Note that we are given the following data from toc.csv in the ubuntu dataset, so no need to get it again\n",
    "#lines,words,characters,filename\n",
    "#9212878,91660344,996253904,dialogueText_196.csv\n",
    "#16587831,166392849,1799936480,dialogueText_301.csv\n",
    "#1038325,11035331,116070597,dialogueText.csv\n",
    "\n",
    "# Note, I used 'Counter' in my mid-module assignment to get the most popular words.\n",
    "# I think you'll need a quick and dirty spaCy tokeniser to rip the utterances list into just a flat list of words.\n",
    "# It's going to be massive, but once you've done that, you can do Counter(flatListOfWords).most_common(20) \n",
    "\n",
    "Log(\"Analyse.py ceased executing at {now}\".format(now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "Log(\"Shell output logged to {file}\".format(file = logFilePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import spacy\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "! spacy download en_core_web_lg'\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ! pip install -q kaggle\n",
    "os.environ['KAGGLE_USERNAME'] = \"avinashfernando\" # username from the json file\n",
    "os.environ['KAGGLE_KEY'] = \"7ba1d8c04bcf3cf93dc66bf86a81f0ab\" # key from the json file\n",
    "!kaggle datasets download -d rtatman/ubuntu-dialogue-corpus\n",
    "! unzip ubuntu-dialogue-corpus.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'path_to_dataset.csv' with the actual path to your CSV file\n",
    "df = pd.read_csv('Ubuntu-dialogue-corpus/dialogueText.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame (head)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the average, minimum, and maximum length of dialogues in terms of the number of turns (rows) present in each dialogue. This will give an overview of how long the conversations usually are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Dialogue Length Analysis\n",
    "dialogue_lengths = df.groupby(\"text\").size()\n",
    "average_dialogue_length = dialogue_lengths.mean()\n",
    "min_dialogue_length = dialogue_lengths.min()\n",
    "max_dialogue_length = dialogue_lengths.max()\n",
    "print(\"Average Dialogue Length:\", average_dialogue_length)\n",
    "print(\"Minimum Dialogue Length:\", min_dialogue_length)\n",
    "print(\"Maximum Dialogue Length:\", max_dialogue_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the distribution of \"from\" and \"to\" fields to identify the most active users in the dataset. This can help in understanding which users are more engaged in the conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. User Interaction Analysis\n",
    "user_activity = df[\"from\"].value_counts()\n",
    "most_active_user = user_activity.idxmax()\n",
    "print(\"Most Active User:\", most_active_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the \"date\" field to determine patterns in dialogue activity over time. This could involve identifying peak hours or days with higher dialogue activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Time-based Analysis\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])  # Convert date column to datetime format\n",
    "df[\"hour\"] = df[\"date\"].dt.hour\n",
    "hourly_activity = df[\"hour\"].value_counts()\n",
    "print(\"Hourly Activity:\\n\", hourly_activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate how many dialogues are complete (i.e., have both \"from\" and \"to\" fields filled for at least two rows) and how many remain incomplete (have only one row or no \"to\" field). This can provide insights into the quality of the dataset and potential data cleaning needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Dialogue Completion Analysis\n",
    "complete_dialogues = df.groupby(\"text\").filter(lambda x: x[\"to\"].count() > 1)\n",
    "incomplete_dialogues = df.groupby(\"text\").filter(lambda x: x[\"to\"].count() <= 1)\n",
    "print(\"Number of Complete Dialogues:\", len(complete_dialogues[\"text\"].unique()))\n",
    "print(\"Number of Incomplete Dialogues:\", len(incomplete_dialogues[\"text\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct a word frequency analysis on the \"text\" field to identify the most common words used by users in their dialogues. This can help in understanding the most prevalent topics of conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Word Frequency Analysis\n",
    "\n",
    "# Preprocess the text to remove unwanted characters and convert to lowercase\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  # Check if the value is a string\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "# Process text with spaCy to obtain lemmatized words and exclude stop words\n",
    "def process_text_with_spacy(text):\n",
    "    if isinstance(text, str):  # Check if the value is a string\n",
    "        doc = nlp(text)\n",
    "        lemmatized_words = [token.lemma_ for token in doc if not token.is_stop]\n",
    "        return lemmatized_words\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Apply preprocessing and processing to the 'text' column\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "df['lemmatized_words'] = df['processed_text'].apply(process_text_with_spacy)\n",
    "\n",
    "# Flatten the list of lemmatized words for word frequency analysis\n",
    "all_words = [word for words_list in df['lemmatized_words'] for word in words_list]\n",
    "\n",
    "# Calculate word frequencies\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# Top N words and their frequencies (change N to any number you desire)\n",
    "top_n = 20\n",
    "top_words, top_word_counts = zip(*word_freq.most_common(top_n))\n",
    "\n",
    "# Plot the word frequency distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_words, top_word_counts)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Top {top_n} Words Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the distribution of dialogue types, we need to analyze the patterns in the \"from\" and \"to\" fields. We can classify the dialogues into different types based on the presence or absence of \"from\" and \"to\" values in each row.\n",
    "\n",
    "QQA (Question-Question-Answer): A dialogue where a user asks a question, another user responds with a question, and finally, a user provides an answer.\n",
    "QAA (Question-Answer-Answer): A dialogue where a user asks a question, and two other users provide separate answers.\n",
    "QA (Question-Answer): A dialogue where a user asks a question, and another user provides an answer.\n",
    "QQ (Question-Question): A dialogue where two users exchange questions without any direct answers.\n",
    "A (Answer): A dialogue where a user provides an answer without any preceding questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Distribution Analysis\n",
    "# Identify dialogue types based on \"from\" and \"to\" fields\n",
    "def classify_dialogue_type(row):\n",
    "    from_user = row[\"from\"]\n",
    "    to_user = row[\"to\"]\n",
    "\n",
    "    if pd.notnull(from_user) and pd.notnull(to_user):\n",
    "        return \"QQA\"\n",
    "    elif pd.notnull(from_user) and pd.isnull(to_user):\n",
    "        return \"QA\"\n",
    "    elif pd.notnull(to_user) and pd.isnull(from_user):\n",
    "        return \"A\"\n",
    "    elif pd.isnull(from_user) and pd.isnull(to_user):\n",
    "        return \"QQ\"\n",
    "\n",
    "# Apply the dialogue type classification to each row\n",
    "df[\"dialogue_type\"] = df.apply(classify_dialogue_type, axis=1)\n",
    "\n",
    "# Calculate the distribution of dialogue types\n",
    "dialogue_type_distribution = df[\"dialogue_type\"].value_counts()\n",
    "\n",
    "# Display the distribution\n",
    "print(\"Dialogue Type Distribution:\")\n",
    "print(dialogue_type_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the distribution of sentence lengths in the dataset involves calculating the length of each sentence (turn of dialogue) in terms of the number of words or characters and then analyzing the distribution of these lengths across the dataset. This can provide insights into the typical sentence length in the dialogues and help identify any patterns or trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 2: Drop rows with missing text values\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "# Step 3: Preprocess the text to calculate sentence lengths\n",
    "df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "df['char_count'] = df['text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# Step 4: Save distribution of sentence lengths (Word Count) into a CSV file\n",
    "word_count_distribution = df['word_count'].value_counts().sort_index()\n",
    "word_count_distribution.to_csv('word_count_distribution.csv', header=['Frequency'])\n",
    "\n",
    "# Step 5: Save distribution of sentence lengths (Character Count) into a CSV file\n",
    "char_count_distribution = df['char_count'].value_counts().sort_index()\n",
    "char_count_distribution.to_csv('char_count_distribution.csv', header=['Frequency'])\n",
    "\n",
    "# Step 6: Calculate summary statistics for word count and character count\n",
    "summary_word_count = df['word_count'].describe()\n",
    "summary_char_count = df['char_count'].describe()\n",
    "\n",
    "# Step 7: Save summary statistics for word count and character count into CSV files\n",
    "summary_word_count.to_csv('word_count_summary.csv', header=True)\n",
    "summary_char_count.to_csv('char_count_summary.csv', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "liv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
