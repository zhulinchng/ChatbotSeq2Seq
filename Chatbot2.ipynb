{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "5-Wc-YXlL8xf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import Vocab, build_vocab_from_iterator, vocab\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from collections import Counter\n",
        "import os\n",
        "import spacy\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eehgd2mMMLbA",
        "outputId": "2563dd86-9249-4f45-802c-e7d6752b422d"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-25 11:14:45.948742: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-lg==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.5.0) (3.5.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.10.11)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_lg\")"
      ],
      "metadata": {
        "id": "cnVPEaSEMcW9"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " ! pip install -q kaggle"
      ],
      "metadata": {
        "id": "Mmq3im-1Mfqx"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_USERNAME'] = \"iiitomiii\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"f0df173a0107862ad5018d7a1ef47736\" # key from the json file\n",
        "!kaggle datasets download -d rtatman/ubuntu-dialogue-corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCN7KI3ZMjB-",
        "outputId": "87f2c273-e422-43d1-fb9f-18d73004ddac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ubuntu-dialogue-corpus.zip to /content\n",
            "100% 799M/799M [00:27<00:00, 39.0MB/s]\n",
            "100% 799M/799M [00:27<00:00, 30.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip ubuntu-dialogue-corpus.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rb1M-WWTPeOB",
        "outputId": "794586c0-01bb-4424-a82a-b2a7da5ce93c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ubuntu-dialogue-corpus.zip\n",
            "  inflating: Ubuntu-dialogue-corpus/dialogueText.csv  \n",
            "  inflating: Ubuntu-dialogue-corpus/dialogueText_196.csv  \n",
            "  inflating: Ubuntu-dialogue-corpus/dialogueText_301.csv  \n",
            "  inflating: toc.csv                 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def question_tokenizer(text):\n",
        "    return [token.text.strip().lower() for token in nlp.tokenizer(text) if not token.is_punct and not token.like_url and token.text.strip() != '']\n",
        "\n",
        "def answer_tokenizer(text):\n",
        "    return [token.text.strip().lower() for token in nlp.tokenizer(text) if not token.is_punct and not token.like_url and token.text.strip() != '']"
      ],
      "metadata": {
        "id": "qRLn_vq2PAXB"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FRM_IDX = 0\n",
        "TO_IDX = 1\n",
        "TEXT_IDX = 2\n",
        "\n",
        "question_answer_pairs = []\n",
        "failed_rows = []\n",
        "with open('Ubuntu-dialogue-corpus/dialogueText.csv', newline='') as csvfile:\n",
        "    dialogue = csv.reader(csvfile)\n",
        "    question = ''\n",
        "    answer = ''\n",
        "    current_dialog_id = ''\n",
        "    current_dialog = []\n",
        "\n",
        "    for idx, row in enumerate(dialogue):\n",
        "      # skip the first row\n",
        "      if idx ==0:\n",
        "        continue\n",
        "\n",
        "      if current_dialog_id != '' and row[1] != current_dialog_id:\n",
        "        # 2 answers 1 question\n",
        "        if(current_dialog[1][TO_IDX] == current_dialog[0][FRM_IDX]\n",
        "           and current_dialog[2][TO_IDX] == current_dialog[0][FRM_IDX]):\n",
        "           question = current_dialog[0][TEXT_IDX]\n",
        "           answer = current_dialog[1][TEXT_IDX]\n",
        "        # 1 answer 2 questions\n",
        "        elif(current_dialog[2][TO_IDX] == current_dialog[1][FRM_IDX]):\n",
        "           question = f'{current_dialog[0][TEXT_IDX]} {current_dialog[1][TEXT_IDX]}'\n",
        "           answer = current_dialog[2][TEXT_IDX]\n",
        "        # 0 answers 3 questions\n",
        "        elif(len(set([current_dialog[0][FRM_IDX],\n",
        "                     current_dialog[1][FRM_IDX],\n",
        "                     current_dialog[2][FRM_IDX]])) == 1):\n",
        "          question = ''\n",
        "          answer = ''\n",
        "        # invalid combination\n",
        "        else:\n",
        "          question = ''\n",
        "          answer = ''\n",
        "          failed_rows.append(current_dialog)\n",
        "\n",
        "        if question != '' and answer != '' \\\n",
        "           and len(answer.split(' ')) < 20 and len(question.split(' ')) < 20:\n",
        "          question_answer_pairs.append((question, answer))\n",
        "        current_dialog = []\n",
        "\n",
        "\n",
        "      current_dialog_id = row[1]\n",
        "      current_dialog.append((row[3], row[4], row[5]))\n",
        "\n",
        "\n",
        "\n",
        "print(len(question_answer_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NeXGmA--Egc",
        "outputId": "2c0fbab0-3302-481f-c9ab-71603e3786ba"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(failed_rows[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rbi2c3hbI2Bt",
        "outputId": "2f3e61bd-02a8-4f4f-e9ab-3f6fda8233a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('psusi', 'maxxism', 'there is no need... the proper drivers will be used with the standard cd'), ('maxxism', 'soundray', ' well I like to play a bit too much.  and sometimes when things get borked a reinstall is easiest'), ('psusi', 'maxxism', 'also you might try using backups... a restore from backup is easier than reinstall')], [('trism', 'sacarlson1', 'https://launchpad.net/ubuntu/+source/linux-lts-backport-natty'), ('sacarlson1', 'jarray52', 'I assume they forgot to add the ppa to add, oh yes as trism just published for us'), ('trism', 'sacarlson1', 'not a ppa, it is in main')], [('DarkMageZ', 'dbe', 'non-free software is available in the multiverse & restricted repositories'), ('dbe', 'DarkAudit', 'Not in UTUTO'), ('DarkMageZ', 'dbe', 'oh, opps :)')], [('selig5', 'ksbalaji', \"it should be 'connect irc.freenode.com'\"), ('ksbalaji', 'tunys', 'done  /connect irc.freenode.net Irssi says Not connected to server.'), ('selig5', 'ksbalaji', \"hey, it's freenode.com not freenode.net\")], [('Seveas', 'Ralc', 'To make xmms or beep-media-player actually play things, change the audio output plugin to eSound'), ('Ralc', 'tobi', 'ill give it a go, thanks :)'), ('Seveas', 'Ralc', 'ubuntuguide is crap...')], [('ipwnu', 'koshari1', 'google etc/rc.local'), ('koshari1', 'CogitoErgoSam', 'given its such a small line can i just add the line to the init.d file?'), ('ipwnu', 'koshari1', ': google etc/rc.local')], [('merkoth', 'k1ko', 'why?'), ('k1ko', 'Gyndawyr', 'the samsung has more contarst'), ('merkoth', 'k1ko', 'I always thought that 80000:1 was better than 20000:1')], [('phong__', 'IdleOne', 'dont work in terminal'), ('IdleOne', 'phong_', 'ctrl+shift+c or v'), ('phong__', 'IdleOne', 'that works ;)')], [('genii', 'andax', '/var/cahe/apt/archives/*     if you remove the dir archives, that will cause issues'), ('andax', 'unop', \"does it break anything? :) i'm doing it since potato came out :)\"), ('genii', 'andax', 'If you remove the directory you just suggested, the apt system cannot find it in order to use it.')], [('erUSUL', 'John_S', 'svn is a source code management system (like cvs and others). So to try a svn version is to try the most up to day code (that fixes bugs but can have others)'), ('John_S', 'jrib', \"hydrogen, kitche, ejer, erUSUL: OK Thanks! That makes much more sense now. So essentially it's like ejer said, more of alpha release since it's the newest changes, correct?\"), ('erUSUL', 'John_S', 'yep; it mainly depends on how the leader manages the project maybe the svn trunk (main branch) is not that alpha and there exists other branches more instables')]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('pairs4.csv','w') as myfile:\n",
        "  wr = csv.writer(myfile) #, quoting=csv.QUOTE_ALL)\n",
        "  for row in question_answer_pairs:\n",
        "    wr.writerow(row)\n"
      ],
      "metadata": {
        "id": "Okoq3dId4ogv"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(question_answer_pairs)\n",
        "#question_answer_pairs = question_answer_pairs[:10]"
      ],
      "metadata": {
        "id": "TCpgpipCcdah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def question_iter():\n",
        "  for question, _ in question_answer_pairs:\n",
        "    yield question_tokenizer(question)\n",
        "\n",
        "def answer_iter():\n",
        "  for _, answer in question_answer_pairs:\n",
        "    yield answer_tokenizer(answer)\n",
        "\n",
        "vocab_q = build_vocab_from_iterator(question_iter(), specials=['<pad>','<bos>','<eos>'])\n",
        "vocab_a = build_vocab_from_iterator(answer_iter(), specials=['<pad>','<bos>','<eos>'])"
      ],
      "metadata": {
        "id": "6riutofYLwhf"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# different way to build vocab.\n",
        "def build_vocab():\n",
        "  counter_q = Counter()\n",
        "  counter_a = Counter()\n",
        "  for question, answer in question_answer_pairs:\n",
        "      counter_q.update(question_tokenizer(question))\n",
        "      counter_a.update(answer_tokenizer(answer))\n",
        "  return vocab(counter_q, specials=['<unk>', '<pad>', '<bos>', '<eos>']), vocab(counter_a, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "\n",
        "\n",
        "vocab_q, vocab_a = build_vocab()"
      ],
      "metadata": {
        "id": "VBhXI7ELHsr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_a['<eos>']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Se8Oybg9t-Ih",
        "outputId": "d607630b-71a3-4ddc-a37a-ada6faa7d9fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max(vocab_q.get_stoi().values())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X58rFQxinnUD",
        "outputId": "3172cfb1-edc9-4656-a392-420a04731648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for question, answer in question_answer_pairs:\n",
        "  question_tensor_ = torch.tensor([vocab_q[token] for token in question_tokenizer(question)],\n",
        "                            dtype=torch.long)\n",
        "  answer_tensor_ = torch.tensor([vocab_a[token] for token in answer_tokenizer(answer)],\n",
        "                            dtype=torch.long)\n",
        "  data.append((question_tensor_, answer_tensor_))"
      ],
      "metadata": {
        "id": "pYm_RpomKgJM"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ASWpoP6yUvi",
        "outputId": "b5e4763c-aedc-4a7d-889e-a01ecdf33d87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device = torch.device('cpu')\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "PAD_IDX = vocab_a['<pad>']\n",
        "BOS_IDX = vocab_a['<bos>']\n",
        "EOS_IDX = vocab_a['<eos>']\n",
        "\n",
        "def generate_batch(data_batch):\n",
        "  batch_question, batch_answer = [], []\n",
        "  for question, answer in data_batch:\n",
        "    batch_answer.append(torch.cat([torch.tensor([BOS_IDX]), answer, torch.tensor([EOS_IDX])], dim=0))\n",
        "    batch_question.append(torch.cat([torch.tensor([BOS_IDX]), question, torch.tensor([EOS_IDX])], dim=0))\n",
        "  batch_answer = pad_sequence(batch_answer, padding_value=PAD_IDX)\n",
        "  batch_question = pad_sequence(batch_question, padding_value=PAD_IDX)\n",
        "  return batch_question, batch_answer\n",
        "\n",
        "train_idx, test_idx = train_test_split(list(range(len(data))), test_size=0.1)\n",
        "train_idx, val_idx = train_test_split(train_idx, test_size=0.1)\n",
        "\n",
        "\n",
        "train_iter = DataLoader(Subset(data, train_idx), batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch)\n",
        "test_iter = DataLoader(Subset(data, test_idx), batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch)\n",
        "val_iter = DataLoader(Subset(data, val_idx), batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch)"
      ],
      "metadata": {
        "id": "Wdk3jw1XKnEY"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, value = next(enumerate(train_iter))"
      ],
      "metadata": {
        "id": "gtnto64ztxmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_q.lookup_tokens([i[0].item() for i in value[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETTfUM7puChS",
        "outputId": "24ef7092-34c2-4ab9-d310-bf3e03eabaf8"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<bos>',\n",
              " 'why',\n",
              " 'is',\n",
              " 'it',\n",
              " 'when',\n",
              " 'i',\n",
              " 'reduce',\n",
              " 'fine',\n",
              " 'programs',\n",
              " 'it',\n",
              " 'says',\n",
              " 'c',\n",
              " 'and',\n",
              " 'webcam',\n",
              " 'are',\n",
              " 'nt',\n",
              " 'found',\n",
              " '<eos>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>']"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_a.lookup_tokens([i[0].item() for i in value[1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17lptpX8u-UL",
        "outputId": "dcb7f501-82bf-4658-b453-d119d05df926"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<bos>',\n",
              " 'what',\n",
              " 'are',\n",
              " 'you',\n",
              " 'trying',\n",
              " 'to',\n",
              " 've',\n",
              " '<eos>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>']"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from binascii import Error\n",
        "import random\n",
        "from typing import Tuple\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim: int,\n",
        "                 emb_dim: int,\n",
        "                 enc_hid_dim: int,\n",
        "                 dec_hid_dim: int,\n",
        "                 dropout: float):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor) -> Tuple[Tensor]:\n",
        "        try:\n",
        "          embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "          outputs, hidden = self.rnn(embedded)\n",
        "\n",
        "          hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "\n",
        "          return outputs, hidden\n",
        "        except Error as e:\n",
        "          print(e)\n",
        "          raise\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 enc_hid_dim: int,\n",
        "                 dec_hid_dim: int,\n",
        "                 attn_dim: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "\n",
        "        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n",
        "\n",
        "        self.attn = nn.Linear(self.attn_in, attn_dim)\n",
        "\n",
        "    def forward(self,\n",
        "                decoder_hidden: Tensor,\n",
        "                encoder_outputs: Tensor) -> Tensor:\n",
        "\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "\n",
        "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "        energy = torch.tanh(self.attn(torch.cat((\n",
        "            repeated_decoder_hidden,\n",
        "            encoder_outputs),\n",
        "            dim = 2)))\n",
        "\n",
        "        attention = torch.sum(energy, dim=2)\n",
        "\n",
        "        return F.softmax(attention, dim=1)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 output_dim: int,\n",
        "                 emb_dim: int,\n",
        "                 enc_hid_dim: int,\n",
        "                 dec_hid_dim: int,\n",
        "                 dropout: int,\n",
        "                 attention: nn.Module):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dropout = dropout\n",
        "        self.attention = attention\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "\n",
        "        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def _weighted_encoder_rep(self,\n",
        "                              decoder_hidden: Tensor,\n",
        "                              encoder_outputs: Tensor) -> Tensor:\n",
        "\n",
        "        a = self.attention(decoder_hidden, encoder_outputs)\n",
        "\n",
        "        a = a.unsqueeze(1)\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n",
        "\n",
        "        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n",
        "\n",
        "        return weighted_encoder_rep\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "                input: Tensor,\n",
        "                decoder_hidden: Tensor,\n",
        "                encoder_outputs: Tensor) -> Tuple[Tensor]:\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,\n",
        "                                                          encoder_outputs)\n",
        "\n",
        "        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n",
        "\n",
        "        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n",
        "\n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n",
        "\n",
        "        output = self.out(torch.cat((output,\n",
        "                                     weighted_encoder_rep,\n",
        "                                     embedded), dim = 1))\n",
        "\n",
        "        return output, decoder_hidden.squeeze(0)\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self,\n",
        "                 encoder: nn.Module,\n",
        "                 decoder: nn.Module,\n",
        "                 device: torch.device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                teacher_forcing_ratio: float = 0.5) -> Tensor:\n",
        "\n",
        "        batch_size = src.shape[1]\n",
        "        max_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "        # first input to the decoder is the <sos> token\n",
        "        output = trg[0,:]\n",
        "\n",
        "        for t in range(1, max_len):\n",
        "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.max(1)[1]\n",
        "            output = (trg[t] if teacher_force else top1)\n",
        "            #print(vocab_a.lookup_token(trg[t][0].item()))\n",
        "        return outputs\n",
        "\n",
        "\n",
        "INPUT_DIM = len(vocab_q)\n",
        "OUTPUT_DIM = len(vocab_a)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ATTN_DIM = 64\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "#ENC_EMB_DIM = 64\n",
        "#DEC_EMB_DIM = 64\n",
        "#ENC_HID_DIM = 128\n",
        "#DEC_HID_DIM = 128\n",
        "#ATTN_DIM = 16\n",
        "#ENC_DROPOUT = 0.5\n",
        "#DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "\n",
        "def init_weights(m: nn.Module):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "\n",
        "\n",
        "model.apply(init_weights)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "def count_parameters(model: nn.Module):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NACYK9dND1T",
        "outputId": "4d9bbcd0-892d-4664-953d-96f77133d0f9"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 108,736,997 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(OUTPUT_DIM)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDNn4Xffg2EG",
        "outputId": "d380ab32-48fc-4595-dbf0-84c7d377ce88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "69114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_IDX = vocab_a['<pad>']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ],
      "metadata": {
        "id": "3BcchRDWQW0f"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "\n",
        "\n",
        "def train(model: nn.Module,\n",
        "          iterator: torch.utils.data.DataLoader,\n",
        "          optimizer: optim.Optimizer,\n",
        "          criterion: nn.Module,\n",
        "          clip: float):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for _, (src, trg) in enumerate(iterator):\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg)\n",
        "\n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "        trg = trg[1:].view(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model: nn.Module,\n",
        "             iterator: torch.utils.data.DataLoader,\n",
        "             criterion: nn.Module):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for _, (src, trg) in enumerate(iterator):\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def epoch_time(start_time: int,\n",
        "               end_time: int):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, val_iter, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "    torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/model.data')\n",
        "\n",
        "test_loss = evaluate(model, test_iter, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1INbWQFQlv5",
        "outputId": "12c3d240-d501-4e9c-e29a-171f1479d3b9"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 11m 59s\n",
            "\tTrain Loss: 5.779 | Train PPL: 323.472\n",
            "\t Val. Loss: 5.978 |  Val. PPL: 394.653\n",
            "Epoch: 02 | Time: 12m 0s\n",
            "\tTrain Loss: 5.184 | Train PPL: 178.439\n",
            "\t Val. Loss: 5.995 |  Val. PPL: 401.227\n",
            "Epoch: 03 | Time: 12m 8s\n",
            "\tTrain Loss: 4.784 | Train PPL: 119.562\n",
            "\t Val. Loss: 6.158 |  Val. PPL: 472.370\n",
            "Epoch: 04 | Time: 12m 13s\n",
            "\tTrain Loss: 4.448 | Train PPL:  85.462\n",
            "\t Val. Loss: 6.342 |  Val. PPL: 568.185\n",
            "Epoch: 05 | Time: 12m 7s\n",
            "\tTrain Loss: 4.159 | Train PPL:  63.976\n",
            "\t Val. Loss: 6.427 |  Val. PPL: 618.491\n",
            "Epoch: 06 | Time: 12m 10s\n",
            "\tTrain Loss: 3.989 | Train PPL:  53.983\n",
            "\t Val. Loss: 6.536 |  Val. PPL: 689.704\n",
            "Epoch: 07 | Time: 12m 18s\n",
            "\tTrain Loss: 3.849 | Train PPL:  46.969\n",
            "\t Val. Loss: 6.623 |  Val. PPL: 751.879\n",
            "Epoch: 08 | Time: 12m 16s\n",
            "\tTrain Loss: 3.703 | Train PPL:  40.563\n",
            "\t Val. Loss: 6.689 |  Val. PPL: 803.155\n",
            "Epoch: 09 | Time: 12m 10s\n",
            "\tTrain Loss: 3.577 | Train PPL:  35.752\n",
            "\t Val. Loss: 6.848 |  Val. PPL: 941.664\n",
            "Epoch: 10 | Time: 12m 13s\n",
            "\tTrain Loss: 3.469 | Train PPL:  32.110\n",
            "\t Val. Loss: 6.912 |  Val. PPL: 1004.095\n",
            "| Test Loss: 6.900 | Test PPL: 992.007 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/model.data')"
      ],
      "metadata": {
        "id": "VHPME1Wwzuu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = Seq2Seq(*args, **kwargs)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/model.data'))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g18GeN-tW6NG",
        "outputId": "7fdd9d86-6c72-4375-c598-7a03ebf74c38"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(73155, 256)\n",
              "    (rnn): GRU(256, 512, bidirectional=True)\n",
              "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=1536, out_features=64, bias=True)\n",
              "    )\n",
              "    (embedding): Embedding(41125, 256)\n",
              "    (rnn): GRU(1280, 512)\n",
              "    (out): Linear(in_features=1792, out_features=41125, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_response(sentence, src_field, trg_field, model, device, max_len = 50):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if isinstance(sentence, str):\n",
        "      tokens = question_tokenizer(sentence)\n",
        "    else:\n",
        "      tokens = sentence\n",
        "\n",
        "\n",
        "    tokens = ['<bos>'] + tokens + ['<eos>']\n",
        "\n",
        "    src_indexes = [src_field[token] for token in tokens]\n",
        "\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "\n",
        "    src_len = torch.LongTensor([len(src_indexes)])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
        "\n",
        "    trg_indexes = [trg_field['<bos>']]\n",
        "\n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden = model.decoder(trg_tensor, hidden, encoder_outputs)\n",
        "\n",
        "\n",
        "        pred_token = output.argmax(1).item()\n",
        "\n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field['<eos>']:\n",
        "            break\n",
        "    try:\n",
        "      trg_tokens = trg_field.lookup_tokens(trg_indexes)\n",
        "    except:\n",
        "      return []\n",
        "\n",
        "    return trg_tokens[1:]"
      ],
      "metadata": {
        "id": "CISIUEZIS8uG"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_response('is ubuntu good?', vocab_q, vocab_a, model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7k2ttf9f1D8",
        "outputId": "07b9892d-0fdb-492e-9a8b-66aa6606a794"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ubuntu', 'is', 'great', '<eos>']"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "expected_responses = []\n",
        "actual_responses = []\n",
        "\n",
        "def filter_specials(tokens):\n",
        "  return [tok for tok in tokens if tok not in ('<bos>', '<eos>', '<pad>') ]\n",
        "\n",
        "for test in test_iter:\n",
        "\n",
        "  for idx in range(0, 63):\n",
        "\n",
        "    question_tokens = filter_specials(vocab_q.lookup_tokens([i[idx].item() for i in test[0]]))\n",
        "    expected_answer_tokens = filter_specials(vocab_a.lookup_tokens([i[idx].item() for i in test[1]]))\n",
        "    if len(expected_answer_tokens) != 0:\n",
        "      actual_answer_tokens = filter_specials(generate_response(question_tokens, vocab_q, vocab_a, model, device))\n",
        "      expected_responses.append(expected_answer_tokens)\n",
        "      actual_responses.append(actual_answer_tokens)\n",
        "  value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "vqlcnzCQTqZr",
        "outputId": "d779acf0-5e99-4577-91e0-98a725affbd5"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-172-7d3380524835>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m63\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mquestion_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mexpected_answer_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_answer_tokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-172-7d3380524835>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m63\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mquestion_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mexpected_answer_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_answer_tokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 7 is out of bounds for dimension 0 with size 7"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filter_specials(vocab_q.lookup_tokens([i[idx].item() for i in test[1]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SekSDpGUd9S1",
        "outputId": "f9a3fb55-b43b-461f-be9a-b6da4f2bf88d"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['possible',\n",
              " 'can',\n",
              " 'thanks',\n",
              " 'ubuntu',\n",
              " 'kernel',\n",
              " 'been',\n",
              " 'join',\n",
              " 'the',\n",
              " 'brb',\n",
              " 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPCEPsE8i-Ok",
        "outputId": "6d805324-ea6f-490f-fd62-18246fec8695"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['help', 'help', 'help']"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_response('help me', vocab_q, vocab_a, model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5M-C27r3iQMQ",
        "outputId": "9d312656-7677-45b8-ce72-18867739bf20"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['what', \"'s\", 'your', 'issue', '<eos>']"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "bleu_score = corpus_bleu(expected_responses, actual_responses)\n",
        "print(\"BLEU score:\", bleu_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RazTgr4BX9jw",
        "outputId": "eeb8bd85-67a4-43a6-93cf-a3d08ddbf862"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score: 7.088753671315663e-156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exact_match_count = sum([1 if ref == gen else 0 for ref, gen in zip(reference_translations, generated_outputs)])\n",
        "exact_match_ratio = exact_match_count / len(reference_translations) * 100\n",
        "print(\"Exact Match Ratio:\", exact_match_ratio)"
      ],
      "metadata": {
        "id": "dc4d4SHHmhUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wer(r, h):\n",
        "    # Initialize the dynamic programming table\n",
        "    d = [[0] * (len(h) + 1) for _ in range(len(r) + 1)]\n",
        "\n",
        "    for i in range(len(r) + 1):\n",
        "        for j in range(len(h) + 1):\n",
        "            if i == 0:\n",
        "                d[i][j] = j\n",
        "            elif j == 0:\n",
        "                d[i][j] = i\n",
        "            elif r[i - 1] == h[j - 1]:\n",
        "                d[i][j] = d[i - 1][j - 1]\n",
        "            else:\n",
        "                d[i][j] = 1 + min(d[i - 1][j], d[i][j - 1], d[i - 1][j - 1])\n",
        "\n",
        "    return float(d[len(r)][len(h)]) / len(r)\n",
        "\n",
        "wer_score = wer([word for ref in reference_translations for word in nltk.word_tokenize(ref[0])],\n",
        "               [word for gen in generated_outputs for word in nltk.word_tokenize(gen)])\n",
        "print(\"Word Error Rate (WER):\", wer_score)"
      ],
      "metadata": {
        "id": "184m1isLmiBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_token_overlap(reference_tokens, generated_tokens):\n",
        "    common_tokens = set(reference_tokens) & set(generated_tokens)\n",
        "    precision = len(common_tokens) / len(generated_tokens) if len(generated_tokens) > 0 else 0\n",
        "    recall = len(common_tokens) / len(reference_tokens) if len(reference_tokens) > 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    return precision, recall, f1_score\n",
        "\n",
        "all_reference_tokens = [word for ref in reference_translations for word in nltk.word_tokenize(ref[0])]\n",
        "all_generated_tokens = [word for gen in generated_outputs for word in nltk.word_tokenize(gen)]\n",
        "\n",
        "precision, recall, f1_score = calculate_token_overlap(all_reference_tokens, all_generated_tokens)\n",
        "print(\"Token Precision:\", precision)\n",
        "print(\"Token Recall:\", recall)\n",
        "print(\"Token F1-score:\", f1_score)"
      ],
      "metadata": {
        "id": "NV9EXrOQmlG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "reference_translations_tokenized = [[token.lower() for token in nltk.word_tokenize(sentence)] for sentence in [ref[0] for ref in reference_translations]]\n",
        "generated_outputs_tokenized = [[token.lower() for token in nltk.word_tokenize(sentence)] for sentence in generated_outputs]\n",
        "\n",
        "# Train a Word2Vec model on a large corpus (not provided in this example)\n",
        "model = Word2Vec(sentences=reference_translations_tokenized + generated_outputs_tokenized, vector_size=100, window=5, min_count=1, sg=1)\n",
        "\n",
        "# Get word embeddings for each token in the reference translations and generated outputs\n",
        "reference_embeddings = [model.wv[token] for ref in reference_translations_tokenized for token in ref]\n",
        "generated_embeddings = [model.wv[token] for token in generated_outputs_tokenized]\n",
        "\n",
        "# Calculate the semantic similarity (cosine similarity) between the embeddings\n",
        "cosine_similarities = cosine_similarity(reference_embeddings, generated_embeddings)\n",
        "average_cosine_similarity = cosine_similarities.mean()\n",
        "\n",
        "print(\"Average Semantic Similarity (Cosine Similarity):\", average_cosine_similarity)"
      ],
      "metadata": {
        "id": "IF2zqTe6mq1v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}