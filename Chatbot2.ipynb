{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":11697,"status":"ok","timestamp":1689835768578,"user":{"displayName":"Tom w","userId":"17110790552569259437"},"user_tz":-60},"id":"5-Wc-YXlL8xf"},"outputs":[],"source":["import torch\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import Vocab, build_vocab_from_iterator\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader, Subset\n","from collections import Counter\n","import os\n","import spacy\n","import csv\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44252,"status":"ok","timestamp":1689835812827,"user":{"displayName":"Tom w","userId":"17110790552569259437"},"user_tz":-60},"id":"eehgd2mMMLbA","outputId":"6174515c-fee4-4921-a343-64780d417349"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-07-20 06:49:31.419814: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Collecting en-core-web-lg==3.5.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl (587.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.5.0) (3.5.4)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.9)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.10)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.4.6)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.8)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.10.2)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (6.3.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.65.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.22.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.27.1)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.10.11)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (23.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.1.0)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.1.3)\n","Installing collected packages: en-core-web-lg\n","Successfully installed en-core-web-lg-3.5.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_lg')\n"]}],"source":["! spacy download en_core_web_lg"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1642,"status":"ok","timestamp":1689835814459,"user":{"displayName":"Tom w","userId":"17110790552569259437"},"user_tz":-60},"id":"cnVPEaSEMcW9"},"outputs":[],"source":["nlp = spacy.load(\"en_core_web_lg\")"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":5638,"status":"ok","timestamp":1689835820092,"user":{"displayName":"Tom w","userId":"17110790552569259437"},"user_tz":-60},"id":"Mmq3im-1Mfqx"},"outputs":[],"source":[" ! pip install -q kaggle"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35185,"status":"ok","timestamp":1689835855270,"user":{"displayName":"Tom w","userId":"17110790552569259437"},"user_tz":-60},"id":"qCN7KI3ZMjB-","outputId":"f03f9e3d-a4c2-4504-b986-203c6a4bf56f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading ubuntu-dialogue-corpus.zip to /content\n","100% 799M/799M [00:34<00:00, 31.4MB/s]\n","100% 799M/799M [00:34<00:00, 24.5MB/s]\n"]}],"source":["os.environ['KAGGLE_USERNAME'] = \"\" # username from the json file\n","os.environ['KAGGLE_KEY'] = \"\" # key from the json file\n","!kaggle datasets download -d rtatman/ubuntu-dialogue-corpus"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35794,"status":"ok","timestamp":1689835891052,"user":{"displayName":"Tom w","userId":"17110790552569259437"},"user_tz":-60},"id":"Rb1M-WWTPeOB","outputId":"036f7b65-94d4-41c2-967a-cd9a14b8907f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Archive:  ubuntu-dialogue-corpus.zip\n","  inflating: Ubuntu-dialogue-corpus/dialogueText.csv  \n","  inflating: Ubuntu-dialogue-corpus/dialogueText_196.csv  \n","  inflating: Ubuntu-dialogue-corpus/dialogueText_301.csv  \n","  inflating: toc.csv                 \n"]}],"source":["! unzip ubuntu-dialogue-corpus.zip"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1689835891052,"user":{"displayName":"Tom w","userId":"17110790552569259437"},"user_tz":-60},"id":"qRLn_vq2PAXB"},"outputs":[],"source":["def tokenizer(text):\n","    return [tok.text.strip() for tok in nlp.tokenizer(text) if not tok.is_punct]"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1689835891053,"user":{"displayName":"Tom w","userId":"17110790552569259437"},"user_tz":-60},"id":"jWrS1TBtrw0A"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":3481,"status":"ok","timestamp":1689835894527,"user":{"displayName":"Tom w","userId":"17110790552569259437"},"user_tz":-60},"id":"-qrovthhHB1R"},"outputs":[],"source":["\n","question_answer_pairs = []\n","with open('Ubuntu-dialogue-corpus/dialogueText.csv', newline='') as csvfile:\n","    dialogue = csv.reader(csvfile)\n","    question = ''\n","    answer = ''\n","    previous_frm = 'Na'\n","    previous_to = 'Na'\n","    for idx, row in enumerate(dialogue):\n","      # skip the first row\n","      if idx ==0:\n","        continue\n","\n","\n","      frm = row[3]\n","      to = row[4]\n","      text = row[5]\n","\n","      #TODO this logic doesn't account for multiple answers to the samae question\n","\n","      # if the text is to the previous from it's an answer\n","      if to == previous_frm:\n","        answer += f' {text}'\n","      # if the from and to are still the same it's the sam question\n","      elif frm == previous_frm and to == previous_to:\n","        question += f' {text}'\n","      # if the from or to are different we assume it's a new question\n","      elif frm != previous_frm or to != previous_to:\n","        # if the previous question got an answer add it\n","\n","        if answer and len(question.split(' ')) < 20:\n","          question_answer_pairs.append((question, answer))\n","        question = text\n","        answer = ''\n","      else:\n","        raise ValueError(f'unknown state: {frm} - {to} - {previous_frm} -- {previous_to}')\n","\n","      previous_frm = frm\n","      previous_to = to\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":25914,"status":"ok","timestamp":1689835920438,"user":{"displayName":"Tom w","userId":"17110790552569259437"},"user_tz":-60},"id":"6riutofYLwhf"},"outputs":[],"source":["\n","\n","def question_iter():\n","  for question, _ in question_answer_pairs:\n","    yield tokenizer(question)\n","\n","def answer_iter():\n","  for _, answer in question_answer_pairs:\n","    yield tokenizer(answer)\n","\n","vocab_q = build_vocab_from_iterator(question_iter(), specials=['<pad>','<bos>','<eos>'])\n","vocab_a = build_vocab_from_iterator(answer_iter(), specials=['<pad>','<bos>','<eos>'])"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1689835920439,"user":{"displayName":"Tom w","userId":"17110790552569259437"},"user_tz":-60},"id":"Se8Oybg9t-Ih"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1689835920439,"user":{"displayName":"Tom w","userId":"17110790552569259437"},"user_tz":-60},"id":"X58rFQxinnUD","outputId":"5149730f-51b4-4354-b4ec-599f7f980943"},"outputs":[{"data":{"text/plain":["87316"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["max(vocab_q.get_stoi().values())"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":37717,"status":"ok","timestamp":1689835958150,"user":{"displayName":"Tom w","userId":"17110790552569259437"},"user_tz":-60},"id":"pYm_RpomKgJM"},"outputs":[],"source":["data = []\n","for question, answer in question_answer_pairs:\n","  question_tensor_ = torch.tensor([vocab_q[token] for token in tokenizer(question)],\n","                            dtype=torch.long)\n","  answer_tensor_ = torch.tensor([vocab_a[token] for token in tokenizer(answer)],\n","                            dtype=torch.long)\n","  data.append((question_tensor_, answer_tensor_))"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1689835958151,"user":{"displayName":"Tom w","userId":"17110790552569259437"},"user_tz":-60},"id":"Wdk3jw1XKnEY"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","#device = torch.device('cpu')\n","\n","BATCH_SIZE = 64\n","PAD_IDX = vocab_a['<pad>']\n","BOS_IDX = vocab_a['<bos>']\n","EOS_IDX = vocab_a['<eos>']\n","\n","def generate_batch(data_batch):\n","  batch_question, batch_answer = [], []\n","  for (question, answer) in data_batch:\n","    batch_answer.append(torch.cat([torch.tensor([BOS_IDX]), answer, torch.tensor([EOS_IDX])], dim=0))\n","    batch_question.append(torch.cat([torch.tensor([BOS_IDX]), question, torch.tensor([EOS_IDX])], dim=0))\n","  batch_answer = pad_sequence(batch_answer, padding_value=PAD_IDX)\n","  batch_question = pad_sequence(batch_question, padding_value=PAD_IDX)\n","  return batch_question, batch_answer\n","\n","train_idx, test_idx = train_test_split(list(range(len(data))), test_size=0.1)\n","train_idx, val_idx = train_test_split(train_idx, test_size=0.1)\n","\n","\n","train_iter = DataLoader(Subset(data, train_idx), batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n","test_iter = DataLoader(Subset(data, test_idx), batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n","val_iter = DataLoader(Subset(data, val_idx), batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1689835958151,"user":{"displayName":"Tom w","userId":"17110790552569259437"},"user_tz":-60},"id":"GtNuoiv3zrop"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14028,"status":"ok","timestamp":1689835972173,"user":{"displayName":"Tom w","userId":"17110790552569259437"},"user_tz":-60},"id":"3NACYK9dND1T","outputId":"4ad08b4b-1e52-401d-f156-82db311cfd00"},"outputs":[{"name":"stdout","output_type":"stream","text":["input embedding 87317 32\n","output embedding 63280 32\n","The model has 19,147,928 trainable parameters\n"]}],"source":["from binascii import Error\n","import random\n","from typing import Tuple\n","\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch import Tensor\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self,\n","                 input_dim: int,\n","                 emb_dim: int,\n","                 enc_hid_dim: int,\n","                 dec_hid_dim: int,\n","                 dropout: float):\n","        super().__init__()\n","\n","        self.input_dim = input_dim\n","        self.emb_dim = emb_dim\n","        self.enc_hid_dim = enc_hid_dim\n","        self.dec_hid_dim = dec_hid_dim\n","        self.dropout = dropout\n","\n","        print(f'input embedding {input_dim} {emb_dim}')\n","        self.embedding = nn.Embedding(input_dim, emb_dim)\n","\n","        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n","\n","        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self,\n","                src: Tensor) -> Tuple[Tensor]:\n","        try:\n","          embedded = self.dropout(self.embedding(src))\n","\n","          outputs, hidden = self.rnn(embedded)\n","\n","          hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n","\n","          return outputs, hidden\n","        except Error as e:\n","          print(e)\n","          raise\n","\n","\n","class Attention(nn.Module):\n","    def __init__(self,\n","                 enc_hid_dim: int,\n","                 dec_hid_dim: int,\n","                 attn_dim: int):\n","        super().__init__()\n","\n","        self.enc_hid_dim = enc_hid_dim\n","        self.dec_hid_dim = dec_hid_dim\n","\n","        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n","\n","        self.attn = nn.Linear(self.attn_in, attn_dim)\n","\n","    def forward(self,\n","                decoder_hidden: Tensor,\n","                encoder_outputs: Tensor) -> Tensor:\n","\n","        src_len = encoder_outputs.shape[0]\n","\n","        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n","\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","\n","        energy = torch.tanh(self.attn(torch.cat((\n","            repeated_decoder_hidden,\n","            encoder_outputs),\n","            dim = 2)))\n","\n","        attention = torch.sum(energy, dim=2)\n","\n","        return F.softmax(attention, dim=1)\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self,\n","                 output_dim: int,\n","                 emb_dim: int,\n","                 enc_hid_dim: int,\n","                 dec_hid_dim: int,\n","                 dropout: int,\n","                 attention: nn.Module):\n","        super().__init__()\n","\n","        self.emb_dim = emb_dim\n","        self.enc_hid_dim = enc_hid_dim\n","        self.dec_hid_dim = dec_hid_dim\n","        self.output_dim = output_dim\n","        self.dropout = dropout\n","        self.attention = attention\n","\n","        print(f'output embedding {output_dim} {emb_dim}')\n","        self.embedding = nn.Embedding(output_dim, emb_dim)\n","\n","        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n","\n","        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","\n","    def _weighted_encoder_rep(self,\n","                              decoder_hidden: Tensor,\n","                              encoder_outputs: Tensor) -> Tensor:\n","\n","        a = self.attention(decoder_hidden, encoder_outputs)\n","\n","        a = a.unsqueeze(1)\n","\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","\n","        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n","\n","        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n","\n","        return weighted_encoder_rep\n","\n","\n","    def forward(self,\n","                input: Tensor,\n","                decoder_hidden: Tensor,\n","                encoder_outputs: Tensor) -> Tuple[Tensor]:\n","\n","        input = input.unsqueeze(0)\n","\n","        embedded = self.dropout(self.embedding(input))\n","\n","        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,\n","                                                          encoder_outputs)\n","\n","        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n","\n","        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n","\n","        embedded = embedded.squeeze(0)\n","        output = output.squeeze(0)\n","        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n","\n","        output = self.out(torch.cat((output,\n","                                     weighted_encoder_rep,\n","                                     embedded), dim = 1))\n","\n","        return output, decoder_hidden.squeeze(0)\n","\n","\n","class Seq2Seq(nn.Module):\n","    def __init__(self,\n","                 encoder: nn.Module,\n","                 decoder: nn.Module,\n","                 device: torch.device):\n","        super().__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","    def forward(self,\n","                src: Tensor,\n","                trg: Tensor,\n","                teacher_forcing_ratio: float = 0.5) -> Tensor:\n","\n","        batch_size = src.shape[1]\n","        max_len = trg.shape[0]\n","        trg_vocab_size = self.decoder.output_dim\n","\n","        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n","\n","        encoder_outputs, hidden = self.encoder(src)\n","\n","        # first input to the decoder is the <sos> token\n","        output = trg[0,:]\n","\n","        for t in range(1, max_len):\n","            output, hidden = self.decoder(output, hidden, encoder_outputs)\n","            outputs[t] = output\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            top1 = output.max(1)[1]\n","            output = (trg[t] if teacher_force else top1)\n","\n","        return outputs\n","\n","\n","INPUT_DIM = len(vocab_q)\n","OUTPUT_DIM = len(vocab_a)\n","#ENC_EMB_DIM = 256\n","#DEC_EMB_DIM = 256\n","#ENC_HID_DIM = 512\n","#DEC_HID_DIM = 512\n","#ATTN_DIM = 64\n","#ENC_DROPOUT = 0.5\n","#DEC_DROPOUT = 0.5\n","\n","ENC_EMB_DIM = 32\n","DEC_EMB_DIM = 32\n","ENC_HID_DIM = 64\n","DEC_HID_DIM = 64\n","ATTN_DIM = 8\n","ENC_DROPOUT = 0.5\n","DEC_DROPOUT = 0.5\n","\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n","\n","attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n","\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n","\n","model = Seq2Seq(enc, dec, device).to(device)\n","\n","\n","def init_weights(m: nn.Module):\n","    for name, param in m.named_parameters():\n","        if 'weight' in name:\n","            nn.init.normal_(param.data, mean=0, std=0.01)\n","        else:\n","            nn.init.constant_(param.data, 0)\n","\n","\n","model.apply(init_weights)\n","\n","optimizer = optim.Adam(model.parameters())\n","\n","\n","def count_parameters(model: nn.Module):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1689835972173,"user":{"displayName":"Tom w","userId":"17110790552569259437"},"user_tz":-60},"id":"nDNn4Xffg2EG","outputId":"2b87b978-a092-4601-f633-ce55ba1b15d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["87317\n"]}],"source":["print(INPUT_DIM)"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1689835972174,"user":{"displayName":"Tom w","userId":"17110790552569259437"},"user_tz":-60},"id":"3BcchRDWQW0f"},"outputs":[],"source":["PAD_IDX = vocab_a['<pad>']\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D1INbWQFQlv5"},"outputs":[],"source":["import math\n","import time\n","\n","\n","def train(model: nn.Module,\n","          iterator: torch.utils.data.DataLoader,\n","          optimizer: optim.Optimizer,\n","          criterion: nn.Module,\n","          clip: float):\n","\n","    model.train()\n","\n","    epoch_loss = 0\n","\n","    for _, (src, trg) in enumerate(iterator):\n","        src, trg = src.to(device), trg.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        output = model(src, trg)\n","\n","        output = output[1:].view(-1, output.shape[-1])\n","        trg = trg[1:].view(-1)\n","\n","        loss = criterion(output, trg)\n","\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)\n","\n","\n","def evaluate(model: nn.Module,\n","             iterator: torch.utils.data.DataLoader,\n","             criterion: nn.Module):\n","\n","    model.eval()\n","\n","    epoch_loss = 0\n","\n","    with torch.no_grad():\n","\n","        for _, (src, trg) in enumerate(iterator):\n","            src, trg = src.to(device), trg.to(device)\n","\n","            output = model(src, trg, 0) #turn off teacher forcing\n","\n","            output = output[1:].view(-1, output.shape[-1])\n","            trg = trg[1:].view(-1)\n","\n","            loss = criterion(output, trg)\n","\n","            epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)\n","\n","\n","def epoch_time(start_time: int,\n","               end_time: int):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs\n","\n","\n","N_EPOCHS = 1\n","CLIP = 1\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","\n","    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n","    valid_loss = evaluate(model, val_iter, criterion)\n","\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n","\n","test_loss = evaluate(model, test_iter, criterion)\n","\n","print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VHPME1Wwzuu0"},"outputs":[],"source":["torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/model.data')"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOzHAnAE1SJiYC/Ly+6x8wc","gpuType":"V100","mount_file_id":"1CAVDMRw9a_r1JMvqx5AO89sOX-YRjkX4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
